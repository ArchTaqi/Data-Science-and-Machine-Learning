{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Weather data using Pandas, Python, and Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://www.shanelynn.ie/analysis-of-weather-data-using-pandas-python-and-seaborn/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Soruce** [Wunderground.com](https://www.wunderground.com/) provides no API, we have to scrap it. find stations with max data by switching in Year view.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on IDUBLINF3\n",
      "Issue with date: 1-1-2015 for station IDUBLINF3\n",
      "Issue with date: 2-1-2015 for station IDUBLINF3\n",
      "Issue with date: 3-1-2015 for station IDUBLINF3\n",
      "Issue with date: 4-1-2015 for station IDUBLINF3\n",
      "Issue with date: 5-1-2015 for station IDUBLINF3\n",
      "Issue with date: 6-1-2015 for station IDUBLINF3\n",
      "Issue with date: 7-1-2015 for station IDUBLINF3\n",
      "Issue with date: 8-1-2015 for station IDUBLINF3\n",
      "Issue with date: 9-1-2015 for station IDUBLINF3\n",
      "Working on date: 2015-01-10 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 10-1-2015 for station IDUBLINF3\n",
      "Issue with date: 11-1-2015 for station IDUBLINF3\n",
      "Issue with date: 12-1-2015 for station IDUBLINF3\n",
      "Issue with date: 13-1-2015 for station IDUBLINF3\n",
      "Issue with date: 14-1-2015 for station IDUBLINF3\n",
      "Issue with date: 15-1-2015 for station IDUBLINF3\n",
      "Issue with date: 16-1-2015 for station IDUBLINF3\n",
      "Issue with date: 17-1-2015 for station IDUBLINF3\n",
      "Issue with date: 18-1-2015 for station IDUBLINF3\n",
      "Issue with date: 19-1-2015 for station IDUBLINF3\n",
      "Working on date: 2015-01-20 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 20-1-2015 for station IDUBLINF3\n",
      "Issue with date: 21-1-2015 for station IDUBLINF3\n",
      "Issue with date: 22-1-2015 for station IDUBLINF3\n",
      "Issue with date: 23-1-2015 for station IDUBLINF3\n",
      "Issue with date: 24-1-2015 for station IDUBLINF3\n",
      "Issue with date: 25-1-2015 for station IDUBLINF3\n",
      "Issue with date: 26-1-2015 for station IDUBLINF3\n",
      "Issue with date: 27-1-2015 for station IDUBLINF3\n",
      "Issue with date: 28-1-2015 for station IDUBLINF3\n",
      "Issue with date: 29-1-2015 for station IDUBLINF3\n",
      "Working on date: 2015-01-30 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 30-1-2015 for station IDUBLINF3\n",
      "Issue with date: 31-1-2015 for station IDUBLINF3\n",
      "Issue with date: 1-2-2015 for station IDUBLINF3\n",
      "Issue with date: 2-2-2015 for station IDUBLINF3\n",
      "Issue with date: 3-2-2015 for station IDUBLINF3\n",
      "Issue with date: 4-2-2015 for station IDUBLINF3\n",
      "Issue with date: 5-2-2015 for station IDUBLINF3\n",
      "Issue with date: 6-2-2015 for station IDUBLINF3\n",
      "Issue with date: 7-2-2015 for station IDUBLINF3\n",
      "Issue with date: 8-2-2015 for station IDUBLINF3\n",
      "Issue with date: 9-2-2015 for station IDUBLINF3\n",
      "Working on date: 2015-02-10 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 10-2-2015 for station IDUBLINF3\n",
      "Issue with date: 11-2-2015 for station IDUBLINF3\n",
      "Issue with date: 12-2-2015 for station IDUBLINF3\n",
      "Issue with date: 13-2-2015 for station IDUBLINF3\n",
      "Issue with date: 14-2-2015 for station IDUBLINF3\n",
      "Issue with date: 15-2-2015 for station IDUBLINF3\n",
      "Issue with date: 16-2-2015 for station IDUBLINF3\n",
      "Issue with date: 17-2-2015 for station IDUBLINF3\n",
      "Issue with date: 18-2-2015 for station IDUBLINF3\n",
      "Issue with date: 19-2-2015 for station IDUBLINF3\n",
      "Working on date: 2015-02-20 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 20-2-2015 for station IDUBLINF3\n",
      "Issue with date: 21-2-2015 for station IDUBLINF3\n",
      "Issue with date: 22-2-2015 for station IDUBLINF3\n",
      "Issue with date: 23-2-2015 for station IDUBLINF3\n",
      "Issue with date: 24-2-2015 for station IDUBLINF3\n",
      "Issue with date: 25-2-2015 for station IDUBLINF3\n",
      "Issue with date: 26-2-2015 for station IDUBLINF3\n",
      "Issue with date: 27-2-2015 for station IDUBLINF3\n",
      "Issue with date: 28-2-2015 for station IDUBLINF3\n",
      "Issue with date: 1-3-2015 for station IDUBLINF3\n",
      "Issue with date: 2-3-2015 for station IDUBLINF3\n",
      "Issue with date: 3-3-2015 for station IDUBLINF3\n",
      "Issue with date: 4-3-2015 for station IDUBLINF3\n",
      "Issue with date: 5-3-2015 for station IDUBLINF3\n",
      "Issue with date: 6-3-2015 for station IDUBLINF3\n",
      "Issue with date: 7-3-2015 for station IDUBLINF3\n",
      "Issue with date: 8-3-2015 for station IDUBLINF3\n",
      "Issue with date: 9-3-2015 for station IDUBLINF3\n",
      "Working on date: 2015-03-10 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 10-3-2015 for station IDUBLINF3\n",
      "Issue with date: 11-3-2015 for station IDUBLINF3\n",
      "Issue with date: 12-3-2015 for station IDUBLINF3\n",
      "Issue with date: 13-3-2015 for station IDUBLINF3\n",
      "Issue with date: 14-3-2015 for station IDUBLINF3\n",
      "Issue with date: 15-3-2015 for station IDUBLINF3\n",
      "Issue with date: 16-3-2015 for station IDUBLINF3\n",
      "Issue with date: 17-3-2015 for station IDUBLINF3\n",
      "Issue with date: 18-3-2015 for station IDUBLINF3\n",
      "Issue with date: 19-3-2015 for station IDUBLINF3\n",
      "Working on date: 2015-03-20 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 20-3-2015 for station IDUBLINF3\n",
      "Issue with date: 21-3-2015 for station IDUBLINF3\n",
      "Issue with date: 22-3-2015 for station IDUBLINF3\n",
      "Issue with date: 23-3-2015 for station IDUBLINF3\n",
      "Issue with date: 24-3-2015 for station IDUBLINF3\n",
      "Issue with date: 25-3-2015 for station IDUBLINF3\n",
      "Issue with date: 26-3-2015 for station IDUBLINF3\n",
      "Issue with date: 27-3-2015 for station IDUBLINF3\n",
      "Issue with date: 28-3-2015 for station IDUBLINF3\n",
      "Issue with date: 29-3-2015 for station IDUBLINF3\n",
      "Working on date: 2015-03-30 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 30-3-2015 for station IDUBLINF3\n",
      "Issue with date: 31-3-2015 for station IDUBLINF3\n",
      "Issue with date: 1-4-2015 for station IDUBLINF3\n",
      "Issue with date: 2-4-2015 for station IDUBLINF3\n",
      "Issue with date: 3-4-2015 for station IDUBLINF3\n",
      "Issue with date: 4-4-2015 for station IDUBLINF3\n",
      "Issue with date: 5-4-2015 for station IDUBLINF3\n",
      "Issue with date: 6-4-2015 for station IDUBLINF3\n",
      "Issue with date: 7-4-2015 for station IDUBLINF3\n",
      "Issue with date: 8-4-2015 for station IDUBLINF3\n",
      "Issue with date: 9-4-2015 for station IDUBLINF3\n",
      "Working on date: 2015-04-10 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 10-4-2015 for station IDUBLINF3\n",
      "Issue with date: 11-4-2015 for station IDUBLINF3\n",
      "Issue with date: 12-4-2015 for station IDUBLINF3\n",
      "Issue with date: 13-4-2015 for station IDUBLINF3\n",
      "Issue with date: 14-4-2015 for station IDUBLINF3\n",
      "Issue with date: 15-4-2015 for station IDUBLINF3\n",
      "Issue with date: 16-4-2015 for station IDUBLINF3\n",
      "Issue with date: 17-4-2015 for station IDUBLINF3\n",
      "Issue with date: 18-4-2015 for station IDUBLINF3\n",
      "Issue with date: 19-4-2015 for station IDUBLINF3\n",
      "Working on date: 2015-04-20 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 20-4-2015 for station IDUBLINF3\n",
      "Issue with date: 21-4-2015 for station IDUBLINF3\n",
      "Issue with date: 22-4-2015 for station IDUBLINF3\n",
      "Issue with date: 23-4-2015 for station IDUBLINF3\n",
      "Issue with date: 24-4-2015 for station IDUBLINF3\n",
      "Issue with date: 25-4-2015 for station IDUBLINF3\n",
      "Issue with date: 26-4-2015 for station IDUBLINF3\n",
      "Issue with date: 27-4-2015 for station IDUBLINF3\n",
      "Issue with date: 28-4-2015 for station IDUBLINF3\n",
      "Issue with date: 29-4-2015 for station IDUBLINF3\n",
      "Working on date: 2015-04-30 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 30-4-2015 for station IDUBLINF3\n",
      "Issue with date: 1-5-2015 for station IDUBLINF3\n",
      "Issue with date: 2-5-2015 for station IDUBLINF3\n",
      "Issue with date: 3-5-2015 for station IDUBLINF3\n",
      "Issue with date: 4-5-2015 for station IDUBLINF3\n",
      "Issue with date: 5-5-2015 for station IDUBLINF3\n",
      "Issue with date: 6-5-2015 for station IDUBLINF3\n",
      "Issue with date: 7-5-2015 for station IDUBLINF3\n",
      "Issue with date: 8-5-2015 for station IDUBLINF3\n",
      "Issue with date: 9-5-2015 for station IDUBLINF3\n",
      "Working on date: 2015-05-10 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 10-5-2015 for station IDUBLINF3\n",
      "Issue with date: 11-5-2015 for station IDUBLINF3\n",
      "Issue with date: 12-5-2015 for station IDUBLINF3\n",
      "Issue with date: 13-5-2015 for station IDUBLINF3\n",
      "Issue with date: 14-5-2015 for station IDUBLINF3\n",
      "Issue with date: 15-5-2015 for station IDUBLINF3\n",
      "Issue with date: 16-5-2015 for station IDUBLINF3\n",
      "Issue with date: 17-5-2015 for station IDUBLINF3\n",
      "Issue with date: 18-5-2015 for station IDUBLINF3\n",
      "Issue with date: 19-5-2015 for station IDUBLINF3\n",
      "Working on date: 2015-05-20 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 20-5-2015 for station IDUBLINF3\n",
      "Issue with date: 21-5-2015 for station IDUBLINF3\n",
      "Issue with date: 22-5-2015 for station IDUBLINF3\n",
      "Issue with date: 23-5-2015 for station IDUBLINF3\n",
      "Issue with date: 24-5-2015 for station IDUBLINF3\n",
      "Issue with date: 25-5-2015 for station IDUBLINF3\n",
      "Issue with date: 26-5-2015 for station IDUBLINF3\n",
      "Issue with date: 27-5-2015 for station IDUBLINF3\n",
      "Issue with date: 28-5-2015 for station IDUBLINF3\n",
      "Issue with date: 29-5-2015 for station IDUBLINF3\n",
      "Working on date: 2015-05-30 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 30-5-2015 for station IDUBLINF3\n",
      "Issue with date: 31-5-2015 for station IDUBLINF3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue with date: 1-6-2015 for station IDUBLINF3\n",
      "Issue with date: 2-6-2015 for station IDUBLINF3\n",
      "Issue with date: 3-6-2015 for station IDUBLINF3\n",
      "Issue with date: 4-6-2015 for station IDUBLINF3\n",
      "Issue with date: 5-6-2015 for station IDUBLINF3\n",
      "Issue with date: 6-6-2015 for station IDUBLINF3\n",
      "Issue with date: 7-6-2015 for station IDUBLINF3\n",
      "Issue with date: 8-6-2015 for station IDUBLINF3\n",
      "Issue with date: 9-6-2015 for station IDUBLINF3\n",
      "Working on date: 2015-06-10 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 10-6-2015 for station IDUBLINF3\n",
      "Issue with date: 11-6-2015 for station IDUBLINF3\n",
      "Issue with date: 12-6-2015 for station IDUBLINF3\n",
      "Issue with date: 13-6-2015 for station IDUBLINF3\n",
      "Issue with date: 14-6-2015 for station IDUBLINF3\n",
      "Issue with date: 15-6-2015 for station IDUBLINF3\n",
      "Issue with date: 16-6-2015 for station IDUBLINF3\n",
      "Issue with date: 17-6-2015 for station IDUBLINF3\n",
      "Issue with date: 18-6-2015 for station IDUBLINF3\n",
      "Issue with date: 19-6-2015 for station IDUBLINF3\n",
      "Working on date: 2015-06-20 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 20-6-2015 for station IDUBLINF3\n",
      "Issue with date: 21-6-2015 for station IDUBLINF3\n",
      "Issue with date: 22-6-2015 for station IDUBLINF3\n",
      "Issue with date: 23-6-2015 for station IDUBLINF3\n",
      "Issue with date: 24-6-2015 for station IDUBLINF3\n",
      "Issue with date: 25-6-2015 for station IDUBLINF3\n",
      "Issue with date: 26-6-2015 for station IDUBLINF3\n",
      "Issue with date: 27-6-2015 for station IDUBLINF3\n",
      "Issue with date: 28-6-2015 for station IDUBLINF3\n",
      "Issue with date: 29-6-2015 for station IDUBLINF3\n",
      "Working on date: 2015-06-30 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 30-6-2015 for station IDUBLINF3\n",
      "Issue with date: 1-7-2015 for station IDUBLINF3\n",
      "Issue with date: 2-7-2015 for station IDUBLINF3\n",
      "Issue with date: 3-7-2015 for station IDUBLINF3\n",
      "Issue with date: 4-7-2015 for station IDUBLINF3\n",
      "Issue with date: 5-7-2015 for station IDUBLINF3\n",
      "Issue with date: 6-7-2015 for station IDUBLINF3\n",
      "Issue with date: 7-7-2015 for station IDUBLINF3\n",
      "Issue with date: 8-7-2015 for station IDUBLINF3\n",
      "Issue with date: 9-7-2015 for station IDUBLINF3\n",
      "Working on date: 2015-07-10 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 10-7-2015 for station IDUBLINF3\n",
      "Issue with date: 11-7-2015 for station IDUBLINF3\n",
      "Issue with date: 12-7-2015 for station IDUBLINF3\n",
      "Issue with date: 13-7-2015 for station IDUBLINF3\n",
      "Issue with date: 14-7-2015 for station IDUBLINF3\n",
      "Issue with date: 15-7-2015 for station IDUBLINF3\n",
      "Issue with date: 16-7-2015 for station IDUBLINF3\n",
      "Issue with date: 17-7-2015 for station IDUBLINF3\n",
      "Issue with date: 18-7-2015 for station IDUBLINF3\n",
      "Issue with date: 19-7-2015 for station IDUBLINF3\n",
      "Working on date: 2015-07-20 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 20-7-2015 for station IDUBLINF3\n",
      "Issue with date: 21-7-2015 for station IDUBLINF3\n",
      "Issue with date: 22-7-2015 for station IDUBLINF3\n",
      "Issue with date: 23-7-2015 for station IDUBLINF3\n",
      "Issue with date: 24-7-2015 for station IDUBLINF3\n",
      "Issue with date: 25-7-2015 for station IDUBLINF3\n",
      "Issue with date: 26-7-2015 for station IDUBLINF3\n",
      "Issue with date: 27-7-2015 for station IDUBLINF3\n",
      "Issue with date: 28-7-2015 for station IDUBLINF3\n",
      "Issue with date: 29-7-2015 for station IDUBLINF3\n",
      "Working on date: 2015-07-30 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 30-7-2015 for station IDUBLINF3\n",
      "Issue with date: 31-7-2015 for station IDUBLINF3\n",
      "Issue with date: 1-8-2015 for station IDUBLINF3\n",
      "Issue with date: 2-8-2015 for station IDUBLINF3\n",
      "Issue with date: 3-8-2015 for station IDUBLINF3\n",
      "Issue with date: 4-8-2015 for station IDUBLINF3\n",
      "Issue with date: 5-8-2015 for station IDUBLINF3\n",
      "Issue with date: 6-8-2015 for station IDUBLINF3\n",
      "Issue with date: 7-8-2015 for station IDUBLINF3\n",
      "Issue with date: 8-8-2015 for station IDUBLINF3\n",
      "Issue with date: 9-8-2015 for station IDUBLINF3\n",
      "Working on date: 2015-08-10 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 10-8-2015 for station IDUBLINF3\n",
      "Issue with date: 11-8-2015 for station IDUBLINF3\n",
      "Issue with date: 12-8-2015 for station IDUBLINF3\n",
      "Issue with date: 13-8-2015 for station IDUBLINF3\n",
      "Issue with date: 14-8-2015 for station IDUBLINF3\n",
      "Issue with date: 15-8-2015 for station IDUBLINF3\n",
      "Issue with date: 16-8-2015 for station IDUBLINF3\n",
      "Issue with date: 17-8-2015 for station IDUBLINF3\n",
      "Issue with date: 18-8-2015 for station IDUBLINF3\n",
      "Issue with date: 19-8-2015 for station IDUBLINF3\n",
      "Working on date: 2015-08-20 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 20-8-2015 for station IDUBLINF3\n",
      "Issue with date: 21-8-2015 for station IDUBLINF3\n",
      "Issue with date: 22-8-2015 for station IDUBLINF3\n",
      "Issue with date: 23-8-2015 for station IDUBLINF3\n",
      "Issue with date: 24-8-2015 for station IDUBLINF3\n",
      "Issue with date: 25-8-2015 for station IDUBLINF3\n",
      "Issue with date: 26-8-2015 for station IDUBLINF3\n",
      "Issue with date: 27-8-2015 for station IDUBLINF3\n",
      "Issue with date: 28-8-2015 for station IDUBLINF3\n",
      "Issue with date: 29-8-2015 for station IDUBLINF3\n",
      "Working on date: 2015-08-30 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 30-8-2015 for station IDUBLINF3\n",
      "Issue with date: 31-8-2015 for station IDUBLINF3\n",
      "Issue with date: 1-9-2015 for station IDUBLINF3\n",
      "Issue with date: 2-9-2015 for station IDUBLINF3\n",
      "Issue with date: 3-9-2015 for station IDUBLINF3\n",
      "Issue with date: 4-9-2015 for station IDUBLINF3\n",
      "Issue with date: 5-9-2015 for station IDUBLINF3\n",
      "Issue with date: 6-9-2015 for station IDUBLINF3\n",
      "Issue with date: 7-9-2015 for station IDUBLINF3\n",
      "Issue with date: 8-9-2015 for station IDUBLINF3\n",
      "Issue with date: 9-9-2015 for station IDUBLINF3\n",
      "Working on date: 2015-09-10 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 10-9-2015 for station IDUBLINF3\n",
      "Issue with date: 11-9-2015 for station IDUBLINF3\n",
      "Issue with date: 12-9-2015 for station IDUBLINF3\n",
      "Issue with date: 13-9-2015 for station IDUBLINF3\n",
      "Issue with date: 14-9-2015 for station IDUBLINF3\n",
      "Issue with date: 15-9-2015 for station IDUBLINF3\n",
      "Issue with date: 16-9-2015 for station IDUBLINF3\n",
      "Issue with date: 17-9-2015 for station IDUBLINF3\n",
      "Issue with date: 18-9-2015 for station IDUBLINF3\n",
      "Issue with date: 19-9-2015 for station IDUBLINF3\n",
      "Working on date: 2015-09-20 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 20-9-2015 for station IDUBLINF3\n",
      "Issue with date: 21-9-2015 for station IDUBLINF3\n",
      "Issue with date: 22-9-2015 for station IDUBLINF3\n",
      "Issue with date: 23-9-2015 for station IDUBLINF3\n",
      "Issue with date: 24-9-2015 for station IDUBLINF3\n",
      "Issue with date: 25-9-2015 for station IDUBLINF3\n",
      "Issue with date: 26-9-2015 for station IDUBLINF3\n",
      "Issue with date: 27-9-2015 for station IDUBLINF3\n",
      "Issue with date: 28-9-2015 for station IDUBLINF3\n",
      "Issue with date: 29-9-2015 for station IDUBLINF3\n",
      "Working on date: 2015-09-30 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 30-9-2015 for station IDUBLINF3\n",
      "Issue with date: 1-10-2015 for station IDUBLINF3\n",
      "Issue with date: 2-10-2015 for station IDUBLINF3\n",
      "Issue with date: 3-10-2015 for station IDUBLINF3\n",
      "Issue with date: 4-10-2015 for station IDUBLINF3\n",
      "Issue with date: 5-10-2015 for station IDUBLINF3\n",
      "Issue with date: 6-10-2015 for station IDUBLINF3\n",
      "Issue with date: 7-10-2015 for station IDUBLINF3\n",
      "Issue with date: 8-10-2015 for station IDUBLINF3\n",
      "Issue with date: 9-10-2015 for station IDUBLINF3\n",
      "Working on date: 2015-10-10 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 10-10-2015 for station IDUBLINF3\n",
      "Issue with date: 11-10-2015 for station IDUBLINF3\n",
      "Issue with date: 12-10-2015 for station IDUBLINF3\n",
      "Issue with date: 13-10-2015 for station IDUBLINF3\n",
      "Issue with date: 14-10-2015 for station IDUBLINF3\n",
      "Issue with date: 15-10-2015 for station IDUBLINF3\n",
      "Issue with date: 16-10-2015 for station IDUBLINF3\n",
      "Issue with date: 17-10-2015 for station IDUBLINF3\n",
      "Issue with date: 18-10-2015 for station IDUBLINF3\n",
      "Issue with date: 19-10-2015 for station IDUBLINF3\n",
      "Working on date: 2015-10-20 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 20-10-2015 for station IDUBLINF3\n",
      "Issue with date: 21-10-2015 for station IDUBLINF3\n",
      "Issue with date: 22-10-2015 for station IDUBLINF3\n",
      "Issue with date: 23-10-2015 for station IDUBLINF3\n",
      "Issue with date: 24-10-2015 for station IDUBLINF3\n",
      "Issue with date: 25-10-2015 for station IDUBLINF3\n",
      "Issue with date: 26-10-2015 for station IDUBLINF3\n",
      "Issue with date: 27-10-2015 for station IDUBLINF3\n",
      "Issue with date: 28-10-2015 for station IDUBLINF3\n",
      "Issue with date: 29-10-2015 for station IDUBLINF3\n",
      "Working on date: 2015-10-30 00:00:00 for station IDUBLINF3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue with date: 30-10-2015 for station IDUBLINF3\n",
      "Issue with date: 31-10-2015 for station IDUBLINF3\n",
      "Issue with date: 1-11-2015 for station IDUBLINF3\n",
      "Issue with date: 2-11-2015 for station IDUBLINF3\n",
      "Issue with date: 3-11-2015 for station IDUBLINF3\n",
      "Issue with date: 4-11-2015 for station IDUBLINF3\n",
      "Issue with date: 5-11-2015 for station IDUBLINF3\n",
      "Issue with date: 6-11-2015 for station IDUBLINF3\n",
      "Issue with date: 7-11-2015 for station IDUBLINF3\n",
      "Issue with date: 8-11-2015 for station IDUBLINF3\n",
      "Issue with date: 9-11-2015 for station IDUBLINF3\n",
      "Working on date: 2015-11-10 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 10-11-2015 for station IDUBLINF3\n",
      "Issue with date: 11-11-2015 for station IDUBLINF3\n",
      "Issue with date: 12-11-2015 for station IDUBLINF3\n",
      "Issue with date: 13-11-2015 for station IDUBLINF3\n",
      "Issue with date: 14-11-2015 for station IDUBLINF3\n",
      "Issue with date: 15-11-2015 for station IDUBLINF3\n",
      "Issue with date: 16-11-2015 for station IDUBLINF3\n",
      "Issue with date: 17-11-2015 for station IDUBLINF3\n",
      "Issue with date: 18-11-2015 for station IDUBLINF3\n",
      "Issue with date: 19-11-2015 for station IDUBLINF3\n",
      "Working on date: 2015-11-20 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 20-11-2015 for station IDUBLINF3\n",
      "Issue with date: 21-11-2015 for station IDUBLINF3\n",
      "Issue with date: 22-11-2015 for station IDUBLINF3\n",
      "Issue with date: 23-11-2015 for station IDUBLINF3\n",
      "Issue with date: 24-11-2015 for station IDUBLINF3\n",
      "Issue with date: 25-11-2015 for station IDUBLINF3\n",
      "Issue with date: 26-11-2015 for station IDUBLINF3\n",
      "Issue with date: 27-11-2015 for station IDUBLINF3\n",
      "Issue with date: 28-11-2015 for station IDUBLINF3\n",
      "Issue with date: 29-11-2015 for station IDUBLINF3\n",
      "Working on date: 2015-11-30 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 30-11-2015 for station IDUBLINF3\n",
      "Issue with date: 1-12-2015 for station IDUBLINF3\n",
      "Issue with date: 2-12-2015 for station IDUBLINF3\n",
      "Issue with date: 3-12-2015 for station IDUBLINF3\n",
      "Issue with date: 4-12-2015 for station IDUBLINF3\n",
      "Issue with date: 5-12-2015 for station IDUBLINF3\n",
      "Issue with date: 6-12-2015 for station IDUBLINF3\n",
      "Issue with date: 7-12-2015 for station IDUBLINF3\n",
      "Issue with date: 8-12-2015 for station IDUBLINF3\n",
      "Issue with date: 9-12-2015 for station IDUBLINF3\n",
      "Working on date: 2015-12-10 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 10-12-2015 for station IDUBLINF3\n",
      "Issue with date: 11-12-2015 for station IDUBLINF3\n",
      "Issue with date: 12-12-2015 for station IDUBLINF3\n",
      "Issue with date: 13-12-2015 for station IDUBLINF3\n",
      "Issue with date: 14-12-2015 for station IDUBLINF3\n",
      "Issue with date: 15-12-2015 for station IDUBLINF3\n",
      "Issue with date: 16-12-2015 for station IDUBLINF3\n",
      "Issue with date: 17-12-2015 for station IDUBLINF3\n",
      "Issue with date: 18-12-2015 for station IDUBLINF3\n",
      "Issue with date: 19-12-2015 for station IDUBLINF3\n",
      "Working on date: 2015-12-20 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 20-12-2015 for station IDUBLINF3\n",
      "Issue with date: 21-12-2015 for station IDUBLINF3\n",
      "Issue with date: 22-12-2015 for station IDUBLINF3\n",
      "Issue with date: 23-12-2015 for station IDUBLINF3\n",
      "Issue with date: 24-12-2015 for station IDUBLINF3\n",
      "Issue with date: 25-12-2015 for station IDUBLINF3\n",
      "Issue with date: 26-12-2015 for station IDUBLINF3\n",
      "Issue with date: 27-12-2015 for station IDUBLINF3\n",
      "Issue with date: 28-12-2015 for station IDUBLINF3\n",
      "Issue with date: 29-12-2015 for station IDUBLINF3\n",
      "Working on date: 2015-12-30 00:00:00 for station IDUBLINF3\n",
      "Issue with date: 30-12-2015 for station IDUBLINF3\n",
      "Issue with date: 31-12-2015 for station IDUBLINF3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All objects passed were None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c9670de2f8e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweather_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# Finally combine all of the individual days and output to CSV for analysis.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/{}_weather.csv\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    223\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                        copy=copy, sort=sort)\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'All objects passed were None'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;31m# consolidate data & figure out what our result ndim is going to be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: All objects passed were None"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from dateutil import parser, rrule\n",
    "from datetime import datetime, time, date\n",
    "import time\n",
    "\n",
    "def getRainfallData(station, day, month, year):\n",
    "    \"\"\"\n",
    "    Function to return a data frame of minute-level weather data for a single Wunderground PWS station.\n",
    "    \n",
    "    Args:\n",
    "        station (string): Station code from the Wunderground website\n",
    "        day (int): Day of month for which data is requested\n",
    "        month (int): Month for which data is requested\n",
    "        year (int): Year for which data is requested\n",
    "    \n",
    "    Returns:\n",
    "        Pandas Dataframe with weather data for specified station and date.\n",
    "    \"\"\"\n",
    "    url = \"http://www.wunderground.com/weatherstation/WXDailyHistory.asp?ID={station}&day={day}&month={month}&year={year}&graphspan=day&format=1\"\n",
    "    full_url = url.format(station=station, day=day, month=month, year=year)\n",
    "    # Request data from wunderground data\n",
    "    response = requests.get(full_url, headers={'User-agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'})\n",
    "    data = response.text\n",
    "    # remove the excess <br> from the text data\n",
    "    data = data.replace('<br>', '')\n",
    "    # Convert to pandas dataframe (fails if issues with weather station)\n",
    "    try:\n",
    "        dataframe = pd.read_csv(io.StringIO(data), index_col=False)\n",
    "        dataframe['station'] = station\n",
    "    except Exception as e:\n",
    "        print(\"Issue with date: {}-{}-{} for station {}\".format(day,month,year, station))\n",
    "        return None\n",
    "    return dataframe\n",
    "    \n",
    "# Generate a list of all of the dates we want data for\n",
    "start_date = \"2015-01-01\"\n",
    "end_date = \"2015-12-31\"\n",
    "start = parser.parse(start_date)\n",
    "end = parser.parse(end_date)\n",
    "dates = list(rrule.rrule(rrule.DAILY, dtstart=start, until=end))\n",
    "\n",
    "# Create a list of stations here to download data for\n",
    "stations = [\"IDUBLINF3\", \"IDUBLINF2\", \"ICARRAIG2\", \"IGALWAYR2\", \"IBELFAST4\", \"ILONDON59\", \"IILEDEFR28\"]\n",
    "# Set a backoff time in seconds if a request fails\n",
    "backoff_time = 10\n",
    "data = {}\n",
    "\n",
    "# Gather data for each station in turn and save to CSV.\n",
    "for station in stations:\n",
    "    print(\"Working on {}\".format(station))\n",
    "    data[station] = []\n",
    "    for date in dates:\n",
    "        # Print period status update messages\n",
    "        if date.day % 10 == 0:\n",
    "            print(\"Working on date: {} for station {}\".format(date, station))\n",
    "        done = False\n",
    "        while done == False:\n",
    "            try:\n",
    "                weather_data = getRainfallData(station, date.day, date.month, date.year)\n",
    "                done = True\n",
    "            except ConnectionError as e:\n",
    "                # May get rate limited by Wunderground.com, backoff if so.\n",
    "                print(\"Got connection error on {}\".format(date))\n",
    "                print(\"Will retry in {} seconds\".format(backoff_time))\n",
    "                time.sleep(10)\n",
    "        # Add each processed date to the overall data\n",
    "        data[station].append(weather_data)\n",
    "    # Finally combine all of the individual days and output to CSV for analysis.\n",
    "    pd.concat(data[station]).to_csv(\"data/{}_weather.csv\".format(station))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleansing and Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data/IEDINBUR6_weather.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8bd170aea34b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'IEDINBUR6'\u001b[0m \u001b[0;31m# Edinburgh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/{}_weather.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Give the variables some friendlier names and convert types as necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'temp'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TemperatureC'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'data/IEDINBUR6_weather.csv' does not exist"
     ]
    }
   ],
   "source": [
    "station = 'IEDINBUR6' # Edinburgh\n",
    "data_raw = pd.read_csv('data/{}_weather.csv'.format(station))\n",
    "\n",
    "# Give the variables some friendlier names and convert types as necessary.\n",
    "data_raw['temp'] = data_raw['TemperatureC'].astype(float)\n",
    "data_raw['rain'] = data_raw['HourlyPrecipMM'].astype(float)\n",
    "data_raw['total_rain'] = data_raw['dailyrainMM'].astype(float)\n",
    "data_raw['date'] = data_raw['DateUTC'].apply(parser.parse)\n",
    "data_raw['humidity'] = data_raw['Humidity'].astype(float)\n",
    "data_raw['wind_direction'] = data_raw['WindDirectionDegrees']\n",
    "data_raw['wind'] = data_raw['WindSpeedKMH']\n",
    "\n",
    "# Extract out only the data we need.\n",
    "data = data_raw.loc[:, ['date', 'station', 'temp', 'rain', 'total_rain', 'humidity', 'wind']]\n",
    "data = data[(data['date'] >= datetime(2015,1,1)) & (data['date'] <= datetime(2015,12,31))]\n",
    "\n",
    "# There's an issue with some stations that record rainfall ~-2500 where data is missing.\n",
    "if (data['rain'] < -500).sum() > 10:\n",
    "    print(\"There's more than 10 messed up days for {}\".format(station))\n",
    "    \n",
    "# remove the bad samples\n",
    "data = data[data['rain'] > -500]\n",
    "\n",
    "# Assign the \"day\" to every date entry\n",
    "data['day'] = data['date'].apply(lambda x: x.date())\n",
    "\n",
    "# Get the time, day, and hour of each timestamp in the dataset\n",
    "data['time_of_day'] = data['date'].apply(lambda x: x.time())\n",
    "data['day_of_week'] = data['date'].apply(lambda x: x.weekday())    \n",
    "data['hour_of_day'] = data['time_of_day'].apply(lambda x: x.hour)\n",
    "# Mark the month for each entry so we can look at monthly patterns\n",
    "data['month'] = data['date'].apply(lambda x: x.month)\n",
    "\n",
    "# Is each time stamp on a working day (Mon-Fri)\n",
    "data['working_day'] = (data['day_of_week'] >= 0) & (data['day_of_week'] <= 4)\n",
    "\n",
    "# Classify into morning or evening times (assuming travel between 8.15-9am and 5.15-6pm)\n",
    "data['morning'] = (data['time_of_day'] >= time(8,15)) & (data['time_of_day'] <= time(9,0))\n",
    "data['evening'] = (data['time_of_day'] >= time(17,15)) & (data['time_of_day'] <= time(18,0))\n",
    "\n",
    "# If there's any rain at all, mark that!\n",
    "data['raining'] = data['rain'] > 0.0\n",
    "\n",
    "# You get wet cycling if its a working day, and its raining at the travel times!\n",
    "data['get_wet_cycling'] = (data['working_day']) & ((data['morning'] & data['rain']) |\n",
    "                                                   (data['evening'] & data['rain']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data summarisation and aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the working days only and create a daily data set of working days:\n",
    "wet_cycling = data[data['working_day'] == True].groupby('day')['get_wet_cycling'].any()\n",
    "wet_cycling = pd.DataFrame(wet_cycling).reset_index()\n",
    "\n",
    "# Group by month for display - monthly data set for plots.\n",
    "wet_cycling['month'] = wet_cycling['day'].apply(lambda x: x.month)\n",
    "monthly = wet_cycling.groupby('month')['get_wet_cycling'].value_counts().reset_index()\n",
    "monthly.rename(columns={\"get_wet_cycling\":\"Rainy\", 0:\"Days\"}, inplace=True)\n",
    "monthly.replace({\"Rainy\": {True: \"Wet\", False:\"Dry\"}}, inplace=True)    \n",
    "monthly['month_name'] = monthly['month'].apply(lambda x: calendar.month_abbr[x])\n",
    "\n",
    "# Get aggregate stats for each day in the dataset on rain in general - for heatmaps.\n",
    "rainy_days = data.groupby(['day']).agg({\n",
    "        \"rain\": {\"rain\": lambda x: (x > 0.0).any(),\n",
    "                 \"rain_amount\": \"sum\"},\n",
    "        \"total_rain\": {\"total_rain\": \"max\"},\n",
    "        \"get_wet_cycling\": {\"get_wet_cycling\": \"any\"}\n",
    "        })    \n",
    "\n",
    "# clean up the aggregated data to a more easily analysed set:\n",
    "rainy_days.reset_index(drop=False, inplace=True) # remove the 'day' as the index\n",
    "rainy_days.rename(columns={\"\":\"date\"}, inplace=True) # The old index column didn't have a name - add \"date\" as name\n",
    "rainy_days.columns = rainy_days.columns.droplevel(level=0) # The aggregation left us with a multi-index\n",
    "                                                           # Remove the top level of this index.\n",
    "rainy_days['rain'] = rainy_days['rain'].astype(bool)       # Change the \"rain\" column to True/False values\n",
    "\n",
    "# Add the number of rainy hours per day this to the rainy_days dataset.\n",
    "temp = data.groupby([\"day\", \"hour_of_day\"])['raining'].any()\n",
    "temp = temp.groupby(level=[0]).sum().reset_index()\n",
    "temp.rename(columns={'raining': 'hours_raining'}, inplace=True)\n",
    "temp['day'] = temp['day'].apply(lambda x: x.to_datetime().date())\n",
    "rainy_days = rainy_days.merge(temp, left_on='date', right_on='day', how='left')\n",
    "rainy_days.drop('day', axis=1, inplace=True)\n",
    "\n",
    "print \"In the year, there were {} rainy days of {} at {}\".format(rainy_days['rain'].sum(), len(rainy_days), station)    \n",
    "print \"It was wet while cycling {} working days of {} at {}\".format(wet_cycling['get_wet_cycling'].sum(), \n",
    "                                                      len(wet_cycling),\n",
    "                                                     station)\n",
    "print \"You get wet cycling {} % of the time!!\".format(wet_cycling['get_wet_cycling'].sum()*1.0*100/len(wet_cycling))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation using Pandas and Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Barchart of Monthly Rainy Cycles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly plot of rainy days, Number of days monthly when cyclists get wet commuting at typical work times in Dublin, Ireland.\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=2)\n",
    "sns.barplot(x=\"month_name\", y=\"Days\", hue=\"Rainy\", data=monthly.sort_values(['month', 'Rainy']))\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Number of Days\")\n",
    "plt.title(\"Wet or Dry Commuting in {}\".format(station))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heatmaps of Rainfall and Rainy Hours per day**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calmap\n",
    "\n",
    "temp = rainy_days.copy().set_index(pd.DatetimeIndex(analysis['rainy_days']['date']))\n",
    "#temp.set_index('date', inplace=True)\n",
    "fig, ax = calmap.calendarplot(temp['hours_raining'], fig_kws={\"figsize\":(15,4)})\n",
    "plt.title(\"Hours raining\")\n",
    "fig, ax = calmap.calendarplot(temp['total_rain'], fig_kws={\"figsize\":(15,4)})\n",
    "plt.title(\"Total Rainfall Daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploratory Line Plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[['get_wet_cycling', 'total_rain', 'hours_raining']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison of Every City in Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare every city in the dataset, summary stats for each city were calculated in advance and then the plot was generated using the seaborn library. To achieve this as quickly as possible, I wrapped the entire data preparation and cleansing phase described above into a single function called “analyse data”, used this function on each city’s dataset, and extracted out the pieces of information needed for the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_station(data_raw, station):\n",
    "    \"\"\"\n",
    "    Function to analyse weather data for a period from one weather station.\n",
    "    \n",
    "    Args:\n",
    "        data_raw (pd.DataFrame): Pandas Dataframe made from CSV downloaded from wunderground.com\n",
    "        station (String): Name of station being analysed (for comments)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with analysis in keys:\n",
    "            data: Processed and cleansed data\n",
    "            monthly: Monthly aggregated statistics on rainfall etc.\n",
    "            wet_cycling: Data on working days and whether you get wet or not commuting\n",
    "            rainy_days: Daily total rainfall for each day in dataset.\n",
    "    \"\"\"\n",
    "    # Give the variables some friendlier names and convert types as necessary.\n",
    "    data_raw['temp'] = data_raw['TemperatureC'].astype(float)\n",
    "    data_raw['rain'] = data_raw['HourlyPrecipMM'].astype(float)\n",
    "    data_raw['total_rain'] = data_raw['dailyrainMM'].astype(float)\n",
    "    data_raw['date'] = data_raw['DateUTC'].apply(parser.parse)\n",
    "    data_raw['humidity'] = data_raw['Humidity'].astype(float)\n",
    "    data_raw['wind_direction'] = data_raw['WindDirectionDegrees']\n",
    "    data_raw['wind'] = data_raw['WindSpeedKMH']\n",
    "    \n",
    "    # Extract out only the data we need.\n",
    "    data = data_raw.loc[:, ['date', 'station', 'temp', 'rain', 'total_rain', 'humidity', 'wind']]\n",
    "    data = data[(data['date'] >= datetime(2015,1,1)) & (data['date'] <= datetime(2015,12,31))]\n",
    "    \n",
    "    # There's an issue with some stations that record rainfall ~-2500 where data is missing.\n",
    "    if (data['rain'] < -500).sum() > 10:\n",
    "        print(\"There's more than 10 messed up days for {}\".format(station))\n",
    "        \n",
    "    # remove the bad samples\n",
    "    data = data[data['rain'] > -500]\n",
    "\n",
    "    # Assign the \"day\" to every date entry\n",
    "    data['day'] = data['date'].apply(lambda x: x.date())\n",
    "\n",
    "    # Get the time, day, and hour of each timestamp in the dataset\n",
    "    data['time_of_day'] = data['date'].apply(lambda x: x.time())\n",
    "    data['day_of_week'] = data['date'].apply(lambda x: x.weekday())    \n",
    "    data['hour_of_day'] = data['time_of_day'].apply(lambda x: x.hour)\n",
    "    # Mark the month for each entry so we can look at monthly patterns\n",
    "    data['month'] = data['date'].apply(lambda x: x.month)\n",
    "\n",
    "    # Is each time stamp on a working day (Mon-Fri)\n",
    "    data['working_day'] = (data['day_of_week'] >= 0) & (data['day_of_week'] <= 4)\n",
    "\n",
    "    # Classify into morning or evening times (assuming travel between 8.15-9am and 5.15-6pm)\n",
    "    data['morning'] = (data['time_of_day'] >= time(8,15)) & (data['time_of_day'] <= time(9,0))\n",
    "    data['evening'] = (data['time_of_day'] >= time(17,15)) & (data['time_of_day'] <= time(18,0))\n",
    "\n",
    "    # If there's any rain at all, mark that!\n",
    "    data['raining'] = data['rain'] > 0.0\n",
    "\n",
    "    # You get wet cycling if its a working day, and its raining at the travel times!\n",
    "    data['get_wet_cycling'] = (data['working_day']) & ((data['morning'] & data['rain']) |\n",
    "                                                       (data['evening'] & data['rain']))\n",
    "    # Looking at the working days only:\n",
    "    wet_cycling = data[data['working_day'] == True].groupby('day')['get_wet_cycling'].any()\n",
    "    wet_cycling = pd.DataFrame(wet_cycling).reset_index()\n",
    "    \n",
    "    # Group by month for display\n",
    "    wet_cycling['month'] = wet_cycling['day'].apply(lambda x: x.month)\n",
    "    monthly = wet_cycling.groupby('month')['get_wet_cycling'].value_counts().reset_index()\n",
    "    monthly.rename(columns={\"get_wet_cycling\":\"Rainy\", 0:\"Days\"}, inplace=True)\n",
    "    monthly.replace({\"Rainy\": {True: \"Wet\", False:\"Dry\"}}, inplace=True)    \n",
    "    monthly['month_name'] = monthly['month'].apply(lambda x: calendar.month_abbr[x])\n",
    "    \n",
    "    # Get aggregate stats for each day in the dataset.\n",
    "    rainy_days = data.groupby(['day']).agg({\n",
    "            \"rain\": {\"rain\": lambda x: (x > 0.0).any(),\n",
    "                     \"rain_amount\": \"sum\"},\n",
    "            \"total_rain\": {\"total_rain\": \"max\"},\n",
    "            \"get_wet_cycling\": {\"get_wet_cycling\": \"any\"}\n",
    "            })    \n",
    "    rainy_days.reset_index(drop=False, inplace=True)\n",
    "    rainy_days.columns = rainy_days.columns.droplevel(level=0)\n",
    "    rainy_days['rain'] = rainy_days['rain'].astype(bool)\n",
    "    rainy_days.rename(columns={\"\":\"date\"}, inplace=True)               \n",
    "    \n",
    "    # Also get the number of hours per day where its raining, and add this to the rainy_days dataset.\n",
    "    temp = data.groupby([\"day\", \"hour_of_day\"])['raining'].any()\n",
    "    temp = temp.groupby(level=[0]).sum().reset_index()\n",
    "    temp.rename(columns={'raining': 'hours_raining'}, inplace=True)\n",
    "    temp['day'] = temp['day'].apply(lambda x: x.to_datetime().date())\n",
    "    rainy_days = rainy_days.merge(temp, left_on='date', right_on='day', how='left')\n",
    "    rainy_days.drop('day', axis=1, inplace=True)\n",
    "    \n",
    "    print \"In the year, there were {} rainy days of {} at {}\".format(rainy_days['rain'].sum(), len(rainy_days), station)    \n",
    "    print \"It was wet while cycling {} working days of {} at {}\".format(wet_cycling['get_wet_cycling'].sum(), \n",
    "                                                          len(wet_cycling),\n",
    "                                                         station)\n",
    "    print \"You get wet cycling {} % of the time!!\".format(wet_cycling['get_wet_cycling'].sum()*1.0*100/len(wet_cycling))\n",
    "\n",
    "    return {\"data\":data, 'monthly':monthly, \"wet_cycling\":wet_cycling, 'rainy_days': rainy_days}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code was used to individually analyse the raw data for each city in turn. Note that this could be done in a more memory efficient manner by simply saving the aggregate statistics for each city at first rather than loading all into memory. I would recommend that approach if you are dealing with more cities etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up each of the stations into memory.\n",
    "stations = [\n",
    " (\"IAMSTERD55\", \"Amsterdam\"),\n",
    " (\"IBCNORTH17\", \"Vancouver\"),\n",
    " (\"IBELFAST4\", \"Belfast\"),\n",
    " (\"IBERLINB54\", \"Berlin\"),\n",
    " (\"ICOGALWA4\", \"Galway\"),\n",
    " (\"ICOMUNID56\", \"Madrid\"),\n",
    " (\"IDUBLIND35\", \"Dublin\"),\n",
    " (\"ILAZIORO71\", \"Rome\"),\n",
    " (\"ILEDEFRA6\", \"Paris\"),\n",
    " (\"ILONDONL28\", \"London\"),\n",
    " (\"IMUNSTER11\", \"Cork\"),\n",
    " (\"INEWSOUT455\", \"Sydney\"),\n",
    " (\"ISOPAULO61\", \"Sao Paulo\"),\n",
    " (\"IWESTERN99\", \"Cape Town\"),\n",
    " (\"KCASANFR148\", \"San Francisco\"),\n",
    " (\"KNYBROOK40\", \"New York\"),\n",
    " (\"IRENFREW4\", \"Glasgow\"),\n",
    " (\"IENGLAND64\", \"Liverpool\"),\n",
    " ('IEDINBUR6', 'Edinburgh')\n",
    "]\n",
    "data = []\n",
    "for station in stations:\n",
    "   weather = {}\n",
    "   print \"Loading data for station: {}\".format(station[1])\n",
    "   weather['data'] = pd.DataFrame.from_csv(\"data/{}_weather.csv\".format(station[0]))\n",
    "   weather['station'] = station[0]\n",
    "   weather['name'] = station[1]\n",
    "   data.append(weather)\n",
    " \n",
    "for ii in range(len(data)):\n",
    "    print \"Processing data for {}\".format(data[ii]['name'])\n",
    "    data[ii]['result'] = analyse_station(data[ii]['data'], data[ii]['station'])\n",
    " \n",
    "# Now extract the number of wet days, the number of wet cycling days, and the number of wet commutes for a single chart.\n",
    "output = []\n",
    "for ii in range(len(data)):\n",
    "    temp = {\n",
    "            \"total_wet_days\": data[ii]['result']['rainy_days']['rain'].sum(),\n",
    "            \"wet_commutes\": data[ii]['result']['wet_cycling']['get_wet_cycling'].sum(),\n",
    "            \"commutes\": len(data[ii]['result']['wet_cycling']),\n",
    "            \"city\": data[ii]['name']\n",
    "        }\n",
    "    temp['percent_wet_commute'] = (temp['wet_commutes'] *1.0 / temp['commutes'])*100\n",
    "    output.append(temp)\n",
    "output = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step in the process is to actually create the diagram using Seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plot of percentage of wet commutes\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.set_style(\"whitegrid\")    # Set style for seaborn output\n",
    "sns.set_context(\"notebook\", font_scale=2)\n",
    "sns.barplot(x=\"city\", y=\"percent_wet_commute\", data=output.sort_values('percent_wet_commute', ascending=False))\n",
    "plt.xlabel(\"City\")\n",
    "plt.ylabel(\"Percentage of Wet Commutes (%)\")\n",
    "plt.suptitle(\"What percentage of your cycles to work do you need a raincoat?\", y=1.05, fontsize=32)\n",
    "plt.title(\"Based on Wundergroud.com weather data for 2015\", fontsize=18)\n",
    "plt.xticks(rotation=60)\n",
    "plt.savefig(\"images/city_comparison_wet_commutes.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of times you got wet cycling to work in 2015 for cities globally. Galway comes out consistently as one of the wettest places for a cycling commute in the data available, but 2015 was a particularly bad year for Irish weather. Here’s hoping for 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
